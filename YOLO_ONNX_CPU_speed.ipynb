{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SMnIMipdsv9j",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "%%bash\n",
        "pip uninstall -y -q spacy thinc catalogue cymem preshed murmurhash srsly blis \\\n",
        "                    pathy typer wasabi pydantic || true\n",
        "pip install -q --force-reinstall \"numpy==1.26.4\"\n",
        "\n",
        "pip install -q \\\n",
        "  onnx==1.17.0 \\\n",
        "  onnxruntime==1.22.0 \\\n",
        "  onnxslim==0.1.53 \\\n",
        "  onnxconverter-common==1.14.0\n",
        "\n",
        "pip install -q ultralytics pillow requests pandas tabulate"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from ultralytics import YOLO\n",
        "from pathlib import Path\n",
        "\n",
        "PT_MODELS_TO_ENSURE_DOWNLOADED = [\"n\", \"s\", \"m\", \"l\", \"x\"]\n",
        "\n",
        "for tag in PT_MODELS_TO_ENSURE_DOWNLOADED:\n",
        "    pt_filename = f\"yolo11{tag}.pt\"\n",
        "    if not Path(pt_filename).exists():\n",
        "        print(f\"Downloading {pt_filename}...\")\n",
        "        _ = YOLO(pt_filename)\n",
        "\n",
        "    onnx_640_path = Path(f\"yolo11{tag}.onnx\")\n",
        "    if not onnx_640_path.exists() and Path(pt_filename).exists():\n",
        "        print(f\"Exporting {pt_filename} to {onnx_640_path} (640x640)...\")\n",
        "        try:\n",
        "            YOLO(pt_filename).export(format=\"onnx\", imgsz=640, simplify=True, verbose=False)\n",
        "        except Exception as e:\n",
        "            print(f\"Error exporting {pt_filename} to 640x640 ONNX: {e}\")\n",
        "print(\"Base PyTorch models checked/downloaded.\")"
      ],
      "metadata": {
        "id": "bDGL0IOOrdrG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os, io, time, random, requests, numpy as np, pandas as pd\n",
        "from pathlib import Path\n",
        "from PIL import Image\n",
        "import onnx, onnxruntime as ort\n",
        "from onnxruntime.quantization import (\n",
        "    quantize_static, CalibrationDataReader,\n",
        "    QuantType, QuantFormat\n",
        ")\n",
        "from tabulate import tabulate\n",
        "import torch\n",
        "from ultralytics import YOLO\n",
        "\n",
        "TARGET_MODEL_TAG_640 = \"n\"\n",
        "\n",
        "INPUT_SIZE_640 = 640\n",
        "picsum_url_640 = \"https://picsum.photos/seed/{}/640/640.jpg\"\n",
        "calibration_images_640 = []\n",
        "num_calib_images_640 = 128\n",
        "print(f\"Collecting images ({INPUT_SIZE_640}x{INPUT_SIZE_640})...\")\n",
        "while len(calibration_images_640) < num_calib_images_640:\n",
        "    try:\n",
        "        img_bytes = requests.get(picsum_url_640.format(random.randint(0, 99999)), timeout=8).content\n",
        "        img_pil = Image.open(io.BytesIO(img_bytes)).convert(\"RGB\").resize((INPUT_SIZE_640, INPUT_SIZE_640), Image.LANCZOS)\n",
        "        img_arr = np.asarray(img_pil, np.float32).transpose(2, 0, 1) / 255.0\n",
        "        calibration_images_640.append(img_arr[np.newaxis])\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "class CalibrationPicsReader(CalibrationDataReader):\n",
        "    def __init__(self, input_name_str: str, calib_data_list: list):\n",
        "        self.input_name = input_name_str\n",
        "        self.data_list = calib_data_list\n",
        "        self.data_iter = iter([{self.input_name: data.copy()} for data in self.data_list])\n",
        "    def get_next(self):\n",
        "        return next(self.data_iter, None)\n",
        "    def rewind(self):\n",
        "        self.data_iter = iter([{self.input_name: data.copy()} for data in self.data_list])\n",
        "\n",
        "def measure_latency_onnx(ort_session, input_feed_dict, runs=100):\n",
        "    ort_session.run(None, input_feed_dict)\n",
        "    t_start = time.perf_counter()\n",
        "    for _ in range(runs):\n",
        "        ort_session.run(None, input_feed_dict)\n",
        "    return (time.perf_counter() - t_start) * 1000 / runs\n",
        "\n",
        "@torch.no_grad()\n",
        "def measure_latency_pytorch(pt_model, torch_input_tensor, runs=100):\n",
        "    pt_model(torch_input_tensor)\n",
        "    t_start = time.perf_counter()\n",
        "    for _ in range(runs):\n",
        "        pt_model(torch_input_tensor)\n",
        "    return (time.perf_counter() - t_start) * 1000 / runs\n",
        "\n",
        "benchmark_table_data_640 = []\n",
        "if not calibration_images_640:\n",
        "    raise ValueError(\"640x640 data collection failed.\")\n",
        "onnx_sample_input_np_640 = calibration_images_640[0].copy()\n",
        "pytorch_sample_input_tensor_640 = torch.from_numpy(onnx_sample_input_np_640).float().cpu()\n",
        "\n",
        "pt_filename_640 = f\"yolo11{TARGET_MODEL_TAG_640}.pt\"\n",
        "\n",
        "onnx_fp32_filename_640 = f\"yolo11{TARGET_MODEL_TAG_640}.onnx\"\n",
        "onnx_int8_filename_640 = f\"yolo11{TARGET_MODEL_TAG_640}_int8_640.onnx\"\n",
        "\n",
        "print(f\"\\nProcessing 640x640: yolo11{TARGET_MODEL_TAG_640}\")\n",
        "latency_pt_fp32_640 = float('nan')\n",
        "latency_onnx_fp32_640 = float('nan')\n",
        "latency_onnx_int8_640 = float('nan')\n",
        "onnx_input_name_str_640 = None\n",
        "\n",
        "if Path(pt_filename_640).exists():\n",
        "    try:\n",
        "        loaded_pytorch_model_640 = YOLO(pt_filename_640).model.cpu().eval()\n",
        "        latency_pt_fp32_640 = measure_latency_pytorch(loaded_pytorch_model_640, pytorch_sample_input_tensor_640)\n",
        "    except Exception as e: print(f\"  Error PT {pt_filename_640}: {e}\")\n",
        "else: print(f\"  {pt_filename_640} not found.\")\n",
        "\n",
        "if Path(onnx_fp32_filename_640).exists():\n",
        "    try:\n",
        "        session_onnx_fp32_640 = ort.InferenceSession(onnx_fp32_filename_640, providers=[\"CPUExecutionProvider\"])\n",
        "        onnx_input_name_str_640 = session_onnx_fp32_640.get_inputs()[0].name\n",
        "        latency_onnx_fp32_640 = measure_latency_onnx(session_onnx_fp32_640, {onnx_input_name_str_640: onnx_sample_input_np_640})\n",
        "    except Exception as e: print(f\"  Error ONNX FP32 {onnx_fp32_filename_640}: {e}\")\n",
        "else: print(f\"  {onnx_fp32_filename_640} (640x640) not found. Ensure it was exported in Cell 2.\")\n",
        "\n",
        "if onnx_input_name_str_640 and not np.isnan(latency_onnx_fp32_640):\n",
        "    if not Path(onnx_int8_filename_640).exists():\n",
        "        try:\n",
        "            calib_reader_int8_640 = CalibrationPicsReader(onnx_input_name_str_640, [img.copy() for img in calibration_images_640])\n",
        "            quantize_static(\n",
        "                model_input=onnx_fp32_filename_640, model_output=onnx_int8_filename_640,\n",
        "                calibration_data_reader=calib_reader_int8_640,\n",
        "                quant_format=QuantFormat.QOperator, activation_type=QuantType.QUInt8,\n",
        "                weight_type=QuantType.QInt8, op_types_to_quantize=[\"Conv\",\"MatMul\",\"Add\",\"Mul\"]\n",
        "            )\n",
        "        except Exception as e: print(f\"  Error quantizing {onnx_int8_filename_640}: {e}\")\n",
        "    if Path(onnx_int8_filename_640).exists():\n",
        "        try:\n",
        "            session_onnx_int8_640 = ort.InferenceSession(onnx_int8_filename_640, providers=[\"CPUExecutionProvider\"])\n",
        "            latency_onnx_int8_640 = measure_latency_onnx(session_onnx_int8_640, {onnx_input_name_str_640: onnx_sample_input_np_640})\n",
        "        except Exception as e: print(f\"  Error benchmarking ONNX INT8 {onnx_int8_filename_640}: {e}\")\n",
        "    else: print(f\"  {onnx_int8_filename_640} not created.\")\n",
        "\n",
        "benchmark_table_data_640.append({\n",
        "    'Model': f\"11{TARGET_MODEL_TAG_640} (640)\", 'PT FP32': latency_pt_fp32_640,\n",
        "    'ONNX FP32': latency_onnx_fp32_640, 'ONNX INT8': latency_onnx_int8_640,\n",
        "})\n",
        "\n",
        "df_results_640 = pd.DataFrame(benchmark_table_data_640)\n",
        "print(\"\\nLatency summary (640x640 CPU, ms/image):\")\n",
        "print(tabulate(df_results_640, headers='keys', tablefmt='github', floatfmt=\".2f\", showindex=False))"
      ],
      "metadata": {
        "id": "gfcIVolBrdtX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os, io, time, random, requests, numpy as np, pandas as pd\n",
        "from pathlib import Path\n",
        "from PIL import Image\n",
        "\n",
        "import onnx, onnxruntime as ort\n",
        "from onnxconverter_common.float16 import convert_float_to_float16\n",
        "from onnxruntime.quantization import (\n",
        "    quantize_static, CalibrationDataReader,\n",
        "    QuantType, QuantFormat\n",
        ")\n",
        "from tabulate import tabulate\n",
        "\n",
        "import torch\n",
        "from ultralytics import YOLO\n",
        "\n",
        "picsum = \"https://picsum.photos/seed/{}/640/640.jpg\"\n",
        "calib = []\n",
        "while len(calib) < 300:\n",
        "    try:\n",
        "        img_b = requests.get(picsum.format(random.randint(0, 99999)), timeout=8).content\n",
        "        img   = Image.open(io.BytesIO(img_b)).convert(\"RGB\")\n",
        "        img_resized = img.resize((640, 640), Image.LANCZOS)\n",
        "        arr   = np.asarray(img_resized, np.float32).transpose(2, 0, 1) / 255.0\n",
        "        calib.append(arr[np.newaxis])\n",
        "        if len(calib) % 50 == 0:\n",
        "            print(f\"   ...collected {len(calib)}\")\n",
        "    except Exception as e:\n",
        "        pass\n",
        "\n",
        "class Pics(CalibrationDataReader):\n",
        "    def __init__(self, name, data): self.name, self.data = name, list(data)\n",
        "    def get_next(self): return {self.name: self.data.pop(0)} if self.data else None\n",
        "    def rewind(self):\n",
        "        pass\n",
        "\n",
        "def latency_onnx(sess, inp_name, sample_np, runs=100):\n",
        "    sess.run(None, {inp_name: sample_np})\n",
        "    t0 = time.perf_counter()\n",
        "    for _ in range(runs):\n",
        "        sess.run(None, {inp_name: sample_np})\n",
        "    return (time.perf_counter() - t0) / runs * 1000\n",
        "\n",
        "@torch.no_grad()\n",
        "def latency_pytorch(model, sample_tensor, runs=100):\n",
        "    model(sample_tensor)\n",
        "    t0 = time.perf_counter()\n",
        "    for _ in range(runs):\n",
        "        model(sample_tensor)\n",
        "    return (time.perf_counter() - t0) / runs * 1000\n",
        "\n",
        "sizes  = [\"n\", \"s\", \"m\", \"l\", \"x\"]\n",
        "table  = []\n",
        "\n",
        "if not calib:\n",
        "    raise ValueError(\"Calibration data is empty. Cannot proceed with benchmarking.\")\n",
        "sample_np_onnx = calib[0]\n",
        "sample_torch_tensor = torch.from_numpy(sample_np_onnx).float().cpu()\n",
        "\n",
        "\n",
        "for tag in sizes:\n",
        "    pt_model_filename = f\"yolo11{tag}.pt\"\n",
        "    onnx_fp32_filename = f\"yolo11{tag}.onnx\"\n",
        "    onnx_fp16_filename = f\"yolo11{tag}_fp16.onnx\"\n",
        "    onnx_int8_filename = f\"yolo11{tag}_int8.onnx\"\n",
        "    print(f\"\\n── yolo11{tag} ──────────────────────────\")\n",
        "\n",
        "    ms_pt_fp32 = float('nan')\n",
        "    ms_onnx_fp32 = float('nan')\n",
        "    ms_onnx_fp16 = float('nan')\n",
        "    ms_onnx_int8 = float('nan')\n",
        "\n",
        "    if Path(pt_model_filename).exists():\n",
        "        try:\n",
        "            pytorch_model = YOLO(pt_model_filename).model.cpu().eval()\n",
        "            ms_pt_fp32 = latency_pytorch(pytorch_model, sample_torch_tensor)\n",
        "            print(f\"  PT FP32 latency: {ms_pt_fp32:.2f} ms\")\n",
        "        except Exception as e:\n",
        "            print(f\"  Error benchmarking PyTorch model {pt_model_filename}: {e}\")\n",
        "    else:\n",
        "        print(f\"  {pt_model_filename} not found. Skipping PyTorch benchmark.\")\n",
        "\n",
        "\n",
        "    if not Path(onnx_fp32_filename).exists():\n",
        "        print(f\"  {onnx_fp32_filename} not found. Skipping ONNX benchmarks for yolo11{tag}.\")\n",
        "        table.append(dict(model=f\"11{tag}\",\n",
        "                          pt_fp32=ms_pt_fp32,\n",
        "                          onnx_fp32=ms_onnx_fp32, onnx_fp16=ms_onnx_fp16, onnx_int8=ms_onnx_int8,\n",
        "                          f16x=float('nan'), i8x=float('nan')))\n",
        "        continue\n",
        "\n",
        "    try:\n",
        "        sess_fp = ort.InferenceSession(onnx_fp32_filename, providers=[\"CPUExecutionProvider\"])\n",
        "        onnx_input_name = sess_fp.get_inputs()[0].name\n",
        "        ms_onnx_fp32 = latency_onnx(sess_fp, onnx_input_name, sample_np_onnx)\n",
        "        print(f\"  ONNX FP32 latency: {ms_onnx_fp32:.2f} ms\")\n",
        "    except Exception as e:\n",
        "        print(f\"  Error loading/benchmarking ONNX FP32 model {onnx_fp32_filename}: {e}\")\n",
        "\n",
        "    try:\n",
        "        if not Path(onnx_fp16_filename).exists():\n",
        "            print(f\"  Creating {onnx_fp16_filename}...\")\n",
        "            model_fp32_loaded = onnx.load(onnx_fp32_filename)\n",
        "            model_fp16 = convert_float_to_float16(model_fp32_loaded, keep_io_types=True)\n",
        "            onnx.save(model_fp16, onnx_fp16_filename)\n",
        "        sess_f16 = ort.InferenceSession(onnx_fp16_filename, providers=[\"CPUExecutionProvider\"])\n",
        "        ms_onnx_fp16 = latency_onnx(sess_f16, onnx_input_name, sample_np_onnx) # Use same input name\n",
        "        print(f\"  ONNX FP16 latency: {ms_onnx_fp16:.2f} ms\")\n",
        "    except Exception as e:\n",
        "        print(f\"  Error creating/benchmarking ONNX FP16 model {onnx_fp16_filename}: {e}\")\n",
        "\n",
        "\n",
        "    calib_data_for_pics = [item.copy() for item in calib]\n",
        "\n",
        "    try:\n",
        "        if not Path(onnx_int8_filename).exists():\n",
        "            print(f\"  Quantizing to {onnx_int8_filename}...\")\n",
        "            quantize_static(\n",
        "                model_input=onnx_fp32_filename,\n",
        "                model_output=onnx_int8_filename,\n",
        "                calibration_data_reader=Pics(onnx_input_name, calib_data_for_pics),\n",
        "                activation_type=QuantType.QUInt8,\n",
        "                weight_type=QuantType.QInt8,\n",
        "                quant_format=QuantFormat.QOperator,\n",
        "                op_types_to_quantize=[\"Conv\", \"MatMul\", \"Add\", \"Mul\"]\n",
        "            )\n",
        "        sess_i8 = ort.InferenceSession(onnx_int8_filename, providers=[\"CPUExecutionProvider\"])\n",
        "        ms_onnx_int8 = latency_onnx(sess_i8, onnx_input_name, sample_np_onnx)\n",
        "        print(f\"  ONNX INT8 latency: {ms_onnx_int8:.2f} ms\")\n",
        "    except Exception as e:\n",
        "        print(f\"  Error quantizing/benchmarking ONNX INT8 model {onnx_int8_filename}: {e}\")\n",
        "\n",
        "    table.append(dict(model=f\"11{tag}\",\n",
        "                      pt_fp32=ms_pt_fp32,\n",
        "                      onnx_fp32=ms_onnx_fp32,\n",
        "                      onnx_fp16=ms_onnx_fp16,\n",
        "                      onnx_int8=ms_onnx_int8,\n",
        "                      f16x=ms_onnx_fp32/ms_onnx_fp16 if ms_onnx_fp16 > 0 else float('nan'),\n",
        "                      i8x=ms_onnx_fp32/ms_onnx_int8 if ms_onnx_int8 > 0 else float('nan')))\n",
        "\n",
        "df = pd.DataFrame(table)\n",
        "print(\"\\n🕒  Latency summary (CPU, ms / image)\")\n",
        "print(tabulate(df,\n",
        "               headers={\"model\":\"Model\",\n",
        "                        \"pt_fp32\":\"PT FP32\",\n",
        "                        \"onnx_fp32\":\"ONNX FP32\",\n",
        "                        \"onnx_fp16\":\"ONNX FP16\",\n",
        "                        \"onnx_int8\":\"ONNX INT8\"},\n",
        "               floatfmt=\".2f\", showindex=False, tablefmt=\"github\"))"
      ],
      "metadata": {
        "id": "LDibiu52MilK",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 943
        },
        "outputId": "e0f2ba3a-08e8-4372-c3de-55ffcbee8629"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "📸  Collecting calibration images (target: 300)...\n",
            "   ...collected 50\n",
            "   ...collected 100\n",
            "   ...collected 150\n",
            "   ...collected 200\n",
            "   ...collected 250\n",
            "   ...collected 300\n",
            "📸  Collected 300 calibration images.\n",
            "\n",
            "── yolo11n ──────────────────────────\n",
            "  PT FP32 latency: 269.71 ms\n",
            "  yolo11n.onnx not found. Skipping ONNX benchmarks for yolo11n.\n",
            "\n",
            "── yolo11s ──────────────────────────\n",
            "  PT FP32 latency: 623.90 ms\n",
            "  ONNX FP32 latency: 514.79 ms\n",
            "  ONNX FP16 latency: 646.26 ms\n",
            "  ONNX INT8 latency: 343.08 ms\n",
            "\n",
            "── yolo11m ──────────────────────────\n",
            "  PT FP32 latency: 1611.84 ms\n",
            "  ONNX FP32 latency: 1443.63 ms\n",
            "  ONNX FP16 latency: 1733.25 ms\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:root:Please consider to run pre-processing before quantization. Refer to example: https://github.com/microsoft/onnxruntime-inference-examples/blob/main/quantization/image_classification/cpu/ReadMe.md \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Quantizing to yolo11m_int8.onnx...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:root:Please consider pre-processing before quantization. See https://github.com/microsoft/onnxruntime-inference-examples/blob/main/quantization/image_classification/cpu/ReadMe.md \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  ONNX INT8 latency: 900.38 ms\n",
            "\n",
            "── yolo11l ──────────────────────────\n",
            "  PT FP32 latency: 2038.66 ms\n",
            "  ONNX FP32 latency: 1938.46 ms\n",
            "  Creating yolo11l_fp16.onnx...\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-ef533c032d5f>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    120\u001b[0m             \u001b[0monnx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_fp16\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0monnx_fp16_filename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m         \u001b[0msess_f16\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mort\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mInferenceSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0monnx_fp16_filename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproviders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"CPUExecutionProvider\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m         \u001b[0mms_onnx_fp16\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlatency_onnx\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msess_f16\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0monnx_input_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_np_onnx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Use same input name\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    123\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"  ONNX FP16 latency: {ms_onnx_fp16:.2f} ms\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-4-ef533c032d5f>\u001b[0m in \u001b[0;36mlatency_onnx\u001b[0;34m(sess, inp_name, sample_np, runs)\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;31m# ---------- Latency Measurement Functions --------------------------------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mlatency_onnx\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minp_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_np\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mruns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m     \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0minp_name\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0msample_np\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# warm-up\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m     \u001b[0mt0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mperf_counter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mruns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/onnxruntime/capi/onnxruntime_inference_collection.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, output_names, input_feed, run_options)\u001b[0m\n\u001b[1;32m    271\u001b[0m             \u001b[0moutput_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0moutput\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_outputs_meta\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    272\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 273\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_names\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_feed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_options\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    274\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mC\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEPFail\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    275\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_enable_fallback\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}